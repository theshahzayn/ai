{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nnPc-68B_T4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#22k5093\n"
      ],
      "metadata": {
        "id": "orvGqjct_bck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LAB FINAL AI\n"
      ],
      "metadata": {
        "id": "pwNlOmjT_aul"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 1"
      ],
      "metadata": {
        "id": "giXVBlQk_Q9m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHWAsYEh_Oqa",
        "outputId": "27ddf6bd-ca31-4f30-a159-310ef5101c9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best solution: [1, 1, 1, 1, 1]\n",
            "Fitness: 5\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "def fitness(chromosome):\n",
        "    return sum(chromosome)\n",
        "\n",
        "def roulette_wheel_selection(population, fitness_values):\n",
        "    total_fitness = sum(fitness_values)\n",
        "    probabilities = [fitness / total_fitness for fitness in fitness_values]\n",
        "    selected = random.choices(population, probabilities, k=2)\n",
        "    return selected\n",
        "\n",
        "def crossover(parent1, parent2):\n",
        "    crossover_point = random.randint(1, len(parent1) - 1)\n",
        "    child1 = parent1[:crossover_point] + parent2[crossover_point:]\n",
        "    child2 = parent2[:crossover_point] + parent1[crossover_point:]\n",
        "    return child1, child2\n",
        "\n",
        "def mutate(chromosome, mutation_rate):\n",
        "    for i in range(len(chromosome)):\n",
        "        if random.random() < mutation_rate:\n",
        "            chromosome[i] = 1 - chromosome[i]\n",
        "    return chromosome\n",
        "\n",
        "def genetic_algorithm(initial_population, generations, mutation_rate):\n",
        "    population = initial_population\n",
        "    for _ in range(generations):\n",
        "        fitness_values = [fitness(chromosome) for chromosome in population]\n",
        "        parents = roulette_wheel_selection(population, fitness_values)\n",
        "        offspring = [crossover(parents[0], parents[1]) for _ in range(len(population) // 2)]\n",
        "        offspring = [gene for sublist in offspring for gene in sublist]\n",
        "        mutated_offspring = [mutate(chromosome, mutation_rate) for chromosome in offspring]\n",
        "        population = mutated_offspring\n",
        "    return max(population, key=fitness)\n",
        "\n",
        "initial_population = [\n",
        "    [0, 1, 1, 0, 1],\n",
        "    [1, 1, 0, 0, 0],\n",
        "    [0, 1, 0, 0, 0],\n",
        "    [1, 0, 0, 1, 1]\n",
        "]\n",
        "\n",
        "generations = 50\n",
        "mutation_rate = 0.01\n",
        "\n",
        "best_solution = genetic_algorithm(initial_population, generations, mutation_rate)\n",
        "print(\"Best solution:\", best_solution)\n",
        "print(\"Fitness:\", fitness(best_solution))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gMMBMe4g_QCu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 2"
      ],
      "metadata": {
        "id": "S9brUgAk_evM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "class QLearning:\n",
        "    def __init__(self, num_states, num_actions, learning_rate=0.1, discount_factor=0.9, exploration_rate=0.1):\n",
        "        self.num_states = num_states\n",
        "        self.num_actions = num_actions\n",
        "        self.learning_rate = learning_rate\n",
        "        self.discount_factor = discount_factor\n",
        "        self.exploration_rate = exploration_rate\n",
        "        self.q_table = np.zeros((num_states, num_actions))\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        if np.random.uniform(0, 1) < self.exploration_rate:\n",
        "            return np.random.choice(self.num_actions)\n",
        "        else:\n",
        "            return np.argmax(self.q_table[state])\n",
        "\n",
        "    def update_q_table(self, state, action, reward, next_state):\n",
        "        old_q_value = self.q_table[state, action]\n",
        "        max_next_q_value = np.max(self.q_table[next_state])\n",
        "        new_q_value = old_q_value + self.learning_rate * (reward + self.discount_factor * max_next_q_value - old_q_value)\n",
        "        self.q_table[state, action] = new_q_value\n",
        "\n",
        "env = gym.make('Taxi-v3')\n",
        "agent = QLearning(env.observation_space.n, env.action_space.n)\n",
        "\n",
        "# Training loop\n",
        "num_episodes = 1000\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "\n",
        "    while not done:\n",
        "        action = agent.choose_action(state)\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        agent.update_q_table(state, action, reward, next_state)\n",
        "        total_reward += reward\n",
        "        state = next_state\n",
        "\n",
        "    if episode % 100 == 0:\n",
        "        print(\"Episode:\", episode, \"Total Reward:\", total_reward)\n",
        "\n",
        "state = env.reset()\n",
        "done = False\n",
        "while not done:\n",
        "    action = np.argmax(agent.q_table[state])\n",
        "    state, reward, done, info = env.step(action)\n",
        "    env.render()\n",
        "\n",
        "env.close()\n"
      ],
      "metadata": {
        "id": "YvoMqqaa_Q0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1hE5zyxYAlne"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "class QLearning:\n",
        "    def __init__(self, num_states, num_actions, learning_rate=0.1, discount_factor=0.9, exploration_rate=0.1):\n",
        "        self.num_states = num_states\n",
        "        self.num_actions = num_actions\n",
        "        self.learning_rate = learning_rate\n",
        "        self.discount_factor = discount_factor\n",
        "        self.exploration_rate = exploration_rate\n",
        "        self.q_table = np.zeros((num_states, num_actions))\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        if np.random.uniform(0, 1) < self.exploration_rate:\n",
        "            return np.random.choice(self.num_actions)\n",
        "        else:\n",
        "            return np.argmax(self.q_table[state])\n",
        "\n",
        "    def update_q_table(self, state, action, reward, next_state):\n",
        "        old_q_value = self.q_table[state, action]\n",
        "        max_next_q_value = np.max(self.q_table[next_state])\n",
        "        new_q_value = old_q_value + self.learning_rate * (reward + self.discount_factor * max_next_q_value - old_q_value)\n",
        "        self.q_table[state, action] = new_q_value\n",
        "\n",
        "env = gym.make('CartPole-v1')\n",
        "num_states = env.observation_space.shape[0]\n",
        "num_actions = env.action_space.n\n",
        "agent = QLearning(num_states, num_actions)\n",
        "\n",
        "num_episodes = 1000\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "\n",
        "    while not done:\n",
        "        action = agent.choose_action(state)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        agent.update_q_table(state, action, reward, next_state)\n",
        "        total_reward += reward\n",
        "        state = next_state\n",
        "\n",
        "    if episode % 100 == 0:\n",
        "        print(\"Episode:\", episode, \"Total Reward:\", total_reward)\n",
        "\n",
        "env.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "vizlIpaGAmz6",
        "outputId": "bb1c5c4c-7bb6-472f-d45c-aa4e3985605b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "arrays used as indices must be of integer (or boolean) type",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-54f9ee0073d0>\u001b[0m in \u001b[0;36m<cell line: 31>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_q_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-54f9ee0073d0>\u001b[0m in \u001b[0;36mchoose_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate_q_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: arrays used as indices must be of integer (or boolean) type"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l9DD0m2DAo-9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}